{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing that\n",
    "- in every layer we typically have to memorize the input and output tensors in the forward pass, for purpose of calculation in the backward pass\n",
    "- in the backward pass we just about always have to use all input tensors in the forward pass, and the output tensor in addition to that\n",
    "\n",
    "We abstract these common operations out into a base `Module` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "pickle_path = URLs.path('mnist_png')/'mnist_png.pkl'\n",
    "path = untar_data(URLs.MNIST)/'training'\n",
    "\n",
    "if not pickle_path.exists():\n",
    "    pickle_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    ds = DataBlock(\n",
    "        blocks = (ImageBlock(PILImageBW), CategoryBlock),\n",
    "        get_items = get_image_files,\n",
    "        get_y = parent_label,\n",
    "        splitter = RandomSplitter(1/6, seed=0)\n",
    "    ).datasets(path)\n",
    "\n",
    "    xs, ys = zip(*ds.train, *ds.valid)\n",
    "    xs = np.stack(L(map(lambda x: np.array(x, dtype=np.float32).reshape(-1), xs))) / 255.\n",
    "    ys = np.array(ys, dtype=np.int64)\n",
    "\n",
    "    x_train, x_valid = xs[:len(ds.train)], xs[len(ds.train):]\n",
    "    y_train, y_valid = ys[:len(ds.train)], ys[len(ds.train):]\n",
    "\n",
    "    save_pickle(pickle_path, [x_train, y_train, x_valid, y_valid])\n",
    "\n",
    "    del ds, xs, ys, x_train, y_train, x_valid, y_valid\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = map(tensor, load_pickle(pickle_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, m = x_train.shape\n",
    "c = y_train.max() + 1\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.randn(m, nh)\n",
    "b1 = torch.zeros(nh)\n",
    "w2 = torch.randn(nh, 1)\n",
    "b2 = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first recompute the model defined in the previous notebook for use in comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(x, w, b):\n",
    "    return x @ w + b\n",
    "def relu(x):\n",
    "    return x.clamp_min(0.)\n",
    "def lin_grad(inp, out, w, b):\n",
    "    inp.g = out.g @ w.t()\n",
    "    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n",
    "    b.g = out.g.sum(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(inp, targ):\n",
    "    l1 = lin(inp, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    out = lin(l2, w2, b2)\n",
    "    diff = out[:, 0] - targ\n",
    "    loss = diff.pow(2).mean()\n",
    "    \n",
    "    out.g = 2. * diff[:, None] / inp.shape[0]\n",
    "    lin_grad(l2, out, w2, b2)\n",
    "    l1.g = (l1 > 0).float() * l2.g\n",
    "    lin_grad(inp, l1, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(inp, targ):\n",
    "    l1 = lin(inp, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    out = lin(l2, w2, b2)\n",
    "    diff = out[:, 0] - targ\n",
    "    loss = diff.pow(2).mean()\n",
    "    \n",
    "    out.g = 2. * diff[:, None] / inp.shape[0]\n",
    "    lin_grad(l2, out, w2, b2)\n",
    "    l1.g = (l1 > 0).float() * l2.g\n",
    "    lin_grad(inp, l1, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_and_backward(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(x):\n",
    "    return x.g.clone()\n",
    "chks = w1, w2, b1, b2, x_train\n",
    "grads = w1g, w2g, b1g, b2g, ig = tuple(map(get_grad, chks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the refactored architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "    def __call__(self, *args):\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "\n",
    "    def forward(self):\n",
    "        raise Exception('not implemented')\n",
    "    def backward(self):\n",
    "        self.bwd(self.out, *self.args)\n",
    "    def bwd(self):\n",
    "        raise Exception('not implemented')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Module):\n",
    "    def forward(self, inp):\n",
    "        return inp.clamp_min(0.)\n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = (inp > 0).float() * out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin(Module):\n",
    "    def __init__(self, w, b):\n",
    "        self.w, self.b = w, b\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return inp @ self.w + self.b\n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = inp.t() @ self.out.g\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse(Module):\n",
    "    def forward (self, inp, targ):\n",
    "        return (inp.squeeze() - targ).pow(2).mean()\n",
    "    def bwd(self, out, inp, targ):\n",
    "        inp.g = 2 * (inp.squeeze() - targ).unsqueeze(-1) / targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, w1, b1, w2, b2):\n",
    "        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n",
    "        self.loss = Mse()\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x, targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): l.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w1, b1, w2, b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(w2g, w2.g, eps=0.01)\n",
    "test_close(b2g, b2.g, eps=0.01)\n",
    "test_close(w1g, w1.g, eps=0.01)\n",
    "test_close(b1g, b1.g, eps=0.01)\n",
    "test_close(ig, x_train.g, eps=0.01)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
