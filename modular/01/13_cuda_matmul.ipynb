{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas before we vectorized away the innermost loop of matmul, in this version we first focus on parallelizing the outer loops that iterae through the entries of the target array. Each invocation accepts a `grid = (i, j)` parameter to indicate the target coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def matmul(grid, a, b, c):\n",
    "    i, j = grid\n",
    "    if i < c.shape[0] and j < c.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(a.shape[1]):\n",
    "            tmp += a[i, k] * b[k, j]\n",
    "        c[i,j] = tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "import gzip, pickle\n",
    "\n",
    "MNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\n",
    "path_data = Path('data')\n",
    "path_data.mkdir(exist_ok=True)\n",
    "path_gz = path_data/'mnist.pkl.gz'\n",
    "\n",
    "if not path_gz.exists():\n",
    "    urlretrieve(MNIST_URL, path_gz)\n",
    "\n",
    "with gzip.open(path_gz, 'rb') as f:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "\n",
    "x_train,y_train,x_valid,y_valid = map(tensor, (x_train,y_train,x_valid,y_valid))\n",
    "\n",
    "torch.manual_seed(1)\n",
    "weights = torch.randn(784, 10)\n",
    "bias = torch.zeros(10)\n",
    "\n",
    "m1 = x_valid[:5]\n",
    "m2 = weights\n",
    "ar, ac = m1.shape \n",
    "br, bc = m2.shape\n",
    "\n",
    "t1 = torch.zeros(ar, bc)\n",
    "\n",
    "for i in range(ar):         # 5\n",
    "    for j in range(bc):     # 10\n",
    "        for k in range(ac): # 784\n",
    "            t1[i, j] += m1[i, k] * m2[k, j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10.9417,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = torch.zeros(ar, bc)\n",
    "matmul((0, 0), m1, m2, res)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a harness for calling `matmul` on each entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def launch_kernel(kernel, grid_x, grid_y, *args, **kwargs):\n",
    "    for i in range(grid_x):\n",
    "        for j in range(grid_y):\n",
    "            kernel((i, j), *args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give it a go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10.9417,  -0.6844,  -7.0038,  -4.0066,  -2.0857,  -3.3588,   3.9127,\n",
       "          -3.4375, -11.4696,  -2.1153],\n",
       "        [ 14.5430,   5.9977,   2.8914,  -4.0777,   6.5914, -14.7383,  -9.2787,\n",
       "           2.1577, -15.2772,  -2.6758],\n",
       "        [  2.2204,  -3.2171,  -4.7988,  -6.0453,  14.1661,  -8.9824,  -4.7922,\n",
       "          -5.4446, -20.6758,  13.5657],\n",
       "        [ -6.7097,   8.8998,  -7.4611,  -7.8966,   2.6994,  -4.7260, -11.0278,\n",
       "         -12.9776,  -6.4443,   3.6376],\n",
       "        [ -2.4444,  -6.4034,  -2.3984,  -9.0371,  11.1772,  -5.7724,  -8.9214,\n",
       "          -3.7862,  -8.9827,   5.2797]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = torch.zeros(ar, bc)\n",
    "launch_kernel(matmul, ar, bc, m1, m2, res)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `cuda.jit`, the indices are provided by `cuda.grid(2)`. In general, the GPU may attempt to evaluate the function beyond the shape of `c`, so we "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def matmul(a,b,c):\n",
    "    i, j = cuda.grid(2)\n",
    "    if i < c.shape[0] and j < c.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(a.shape[1]): tmp += a[i, k] * b[k, j]\n",
    "        c[i,j] = tmp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We setup the testing environment, and send all the relevant tensors to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "tr = x_train @ weights\n",
    "r = np.zeros(tr.shape)\n",
    "m1g, m2g, rg = map(cuda.to_device, (x_train, weights, r))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall execute 16 x 16 threads for each CUDA block, and compute the number of blocks in each direction that we shall use. If `c` has shape strictly smaller than `blocks * TPB`, then `matmul` will be run on invalid indices, hence the bounds checking in our definition previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3125, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "TPB = 16\n",
    "rr, rc = r.shape\n",
    "blocks = (math.ceil(rr / TPB), math.ceil(rc / TPB))\n",
    "blocks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "matmul[blocks, (TPB, TPB)](m1g, m2g, rg)\n",
    "r = rg.copy_to_host()\n",
    "test_close(tr, r, eps=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.85 ms ± 590 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 10\n",
    "matmul[blocks, (TPB, TPB)](m1g, m2g, rg)\n",
    "r = rg.copy_to_host()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the speed of the inbuilt `matmul` of PyTorch on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "m1c, m2c = x_train.cuda(), weights.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "r = (m1c @ m2c).cpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing performance; we observe a close to 10x speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.05 ms ± 213 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 r = (m1c @ m2c).cpu()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
